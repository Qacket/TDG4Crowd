import os
import io
import json

import pandas as pd
import torch
import numpy as np
from collections import defaultdict
from torch.utils.data import Dataset
from nltk.tokenize import TweetTokenizer

from utils import OrderedCounter


class Relation(Dataset):

    def __init__(self, data_dir, data_path, data_file, create_vocab=False, **kwargs):

        super().__init__()
        self.data_dir = data_dir
        self.max_sequence_length = kwargs.get('max_sequence_length', 60)
        self.min_occ = kwargs.get('min_occ', 3)

        self.raw_data_path = data_dir + data_path
        self.data_file = data_file

        self.vocab_file = '/domain_relation_task.vocab.json'

        if create_vocab:
            print("Creating sentiment vocab.")
            self._create_vocab()
            self._create_data()
        else:
            self._load_vocab()
            print("Creating sentiment data.")
            self._create_data()



    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        idx = str(idx)

        return {
            'task_id': self.data[idx]['task_id'],
            'annotator_id': self.data[idx]['annotator_id'],
            'ground_truth:': self.data[idx]['ground_truth'],
            'answer': self.data[idx]['answer'],
            'input': np.asarray(self.data[idx]['input']),
            'target': np.asarray(self.data[idx]['target']),
            'length': self.data[idx]['length']
        }

    @property
    def vocab_size(self):
        return len(self.w2i)

    @property
    def pad_idx(self):
        return self.w2i['<pad>']

    @property
    def sos_idx(self):
        return self.w2i['<sos>']

    @property
    def eos_idx(self):
        return self.w2i['<eos>']

    @property
    def unk_idx(self):
        return self.w2i['<unk>']

    def get_w2i(self):
        return self.w2i

    def get_i2w(self):
        return self.i2w


    def _create_data(self):


        tokenizer = TweetTokenizer(preserve_case=False)

        data = defaultdict(dict)
        total_data = pd.read_csv(self.raw_data_path)

        for i in range(len(total_data)):

            task_id = total_data.iloc[i, 0]
            annotator_id = total_data.iloc[i, 1]

            words = tokenizer.tokenize(total_data.iloc[i, 2])

            ground_truth = total_data.iloc[i, 3]
            answer = total_data.iloc[i, 4]

            input = ['<sos>'] + words
            input = input[:self.max_sequence_length]

            target = words[:self.max_sequence_length-1]
            target = target + ['<eos>']

            assert len(input) == len(target), "%i, %i"%(len(input), len(target))
            length = len(input)
            input.extend(['<pad>'] * (self.max_sequence_length-length))
            target.extend(['<pad>'] * (self.max_sequence_length-length))

            input = [self.w2i.get(w, self.w2i['<unk>']) for w in input]
            target = [self.w2i.get(w, self.w2i['<unk>']) for w in target]

            id = len(data)
            data[id]['task_id'] = str(task_id)
            data[id]['annotator_id'] = str(annotator_id)
            data[id]['ground_truth'] = str(ground_truth)
            data[id]['answer'] = str(answer)

            data[id]['input'] = input
            data[id]['target'] = target
            data[id]['length'] = length
        with io.open(self.data_dir + self.data_file, 'wb') as data_file:
            data = json.dumps(data, ensure_ascii=False)
            data_file.write(data.encode('utf8', 'replace'))

        self._load_data(vocab=False)

    def _load_data(self, vocab=True):

        with open(self.data_dir + self.data_file, 'r') as file:
            self.data = json.load(file)
        if vocab:
            with open(self.data_dir + self.vocab_file, 'r') as file:
                vocab = json.load(file)
            self.w2i, self.i2w = vocab['w2i'], vocab['i2w']


    def _create_vocab(self):

        tokenizer = TweetTokenizer(preserve_case=False)

        w2c = OrderedCounter()
        w2i = dict()
        i2w = dict()

        special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']
        for st in special_tokens:
            i2w[len(w2i)] = st
            w2i[st] = len(w2i)

        with open(self.raw_data_path, 'r') as file:

            for i, line in enumerate(file):
                words = tokenizer.tokenize(line)
                w2c.update(words)

            for w, c in w2c.items():
                if c > self.min_occ and w not in special_tokens:
                    i2w[len(w2i)] = w
                    w2i[w] = len(w2i)

        assert len(w2i) == len(i2w)

        print("Vocablurary of %i keys created." %len(w2i))

        vocab = dict(w2i=w2i, i2w=i2w)
        with io.open(self.data_dir + self.vocab_file, 'wb') as vocab_file:
            data = json.dumps(vocab, ensure_ascii=False)
            vocab_file.write(data.encode('utf8', 'replace'))

        self._load_vocab()

    def _load_vocab(self):
        with open(self.data_dir + self.vocab_file, 'r') as vocab_file:
            vocab = json.load(vocab_file)

        self.w2i, self.i2w = vocab['w2i'], vocab['i2w']